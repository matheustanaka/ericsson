# Default values for tehChart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

imageCredentials:
  init-wait-for-data:
    repoPath:
    pullPolicy:
    registry:
      url:
  teh:
    repoPath:
    pullPolicy:
    registry:
      url:
  pre-install-job:
    repoPath:
    pullPolicy:
    registry:
      url:

images:
  init-wait-for-data:
    name: "eric-bss-eb-wait-for-condition"
    tag: "22.15.12"
  teh:
    name: "eric-bss-eb-teh"
    tag: "22.15.12"
  pre-install-job:
    name: "eric-bss-eb-teh"
    tag: "22.15.12"

# To generate trace files, it is necessary to set the trace file names in the variables below.
# These files will be stored in the log PVC.
tracingAndLogging:
  # dtaTracefile: "dta.teh.trc"
  dtaTracefile: ""

tnsnames:
  bscsdb: '{{ tpl .Values.global.tnsnames.bscsdb . }}'

sqlnet:
  ora: '{{ tpl .Values.global.sqlnet.ora . }}'

# Grace period for POD termination, in which the POD can shut down gracefully.
# After that period, k8s will terminate the POD un-gracefully (SIGKILL).
# Adapt this value in case the POD needs more time to terminate gracefully,
#   e.g. in case the PID-file remains after POD termination.
# Note, after the POD has terminated gracefully, the terminationGracePeriodSeconds is stopped immediately.
#       So it does not harm to specify a big value here.
terminationGracePeriodSeconds: 60

#DATA_RETRY_TIME, DATA_RETRY_TIME_MAX
#By default, there is no definition for a time interval in which applications try to reconnect to DaTA.
#If you set the environment variable, the value represents the time interval an application waits
#until it tries to reconnect to DaTA.
#Subsequent connection retries are performed under the following conditions:
# - A double time interval has elapsed and has exceeded a maximum of 60 seconds.
# - A double time interval has elapsed and the value specified by DATA_RETRY_TIME_MAX has been exceeded.
dataRetry:
  dataRetryTime:
  dataRetryTimeMax:

# resource limits for parallelization of the initial job
resources:
  limits:
    cpu: 11000m
    memory: 100Gi
  requests:
    cpu: 100m
    memory: 128Mi

#A local nodeSelector definition has priority over the global definition above.
#nodeSelector: {}

# A local affinity definition has priority over the global definition.
# affinity: {}
# Example:
# affinity:
#   nodeAffinity:
#     preferredDuringSchedulingIgnoredDuringExecution:
#     - weight: 50
#       preference:
#         matchExpressions:
#         - key: topology.kubernetes.io/zone
#           operator: In
#           values:
#           - us-east-1a
#     - weight: 50
#       preference:
#         matchExpressions:
#         - key: topology.kubernetes.io/zone
#           operator: In
#           values:
#           - us-east-1b

# Start parameters for the initial PTEH job.
# Use parallelization for the initial PTEH job
initialPtehStartParameters: "-c -p -n 10"

# Specify whether the initial TEH Job (generation of XREFs) should be executed on helm install / upgrade
runInitialTehJob:
  onHelmInstall: true
  onHelmUpgrade: false

CronJob:
  failedJobsHistoryLimit: 1
  successfulJobsHistoryLimit: 3

# Add new job group starting with "- name" for each TEH batch job you want to run
# Job names must follow DNS naming convention: lower case alphanumeric characters, '-' or '.'
# See en.wikipedia.org/wiki/Cron for coding of schedule time
# (see commented examples)
# default settings allow for parallelization of the cron jobs
tehCronJobs:
  - name: "pteh-cxref-generation"
    startParameters: "-s -p -n 10"
    schedule: "30 1 */5 * *"
  - name: "pteh-non-cxref-generation"
    startParameters: "-r"
    schedule: "0 0 * * *"
