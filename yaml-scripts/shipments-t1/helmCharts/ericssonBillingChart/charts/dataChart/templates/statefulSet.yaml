apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "{{ .Values.metadataName }}"
  labels:
    app:  eric-bss-eb-data
    chart: {{ template "eric-bss-eb-data.chart" . }}
    {{- include "eric-bss-eb-data.kubernetesIoInfo" . | nindent 4 }}
    heritage: {{ .Release.Service }}
  annotations:
  {{- include "eric-bss-eb-data.productInfo" . | indent 4 }}
spec:
  serviceName: eric-bss-eb-data-service
  {{ if .Values.global.dataHighAvailability.enable }}
  {{ if gt (int .Values.global.dataHighAvailability.replicaCount) 2 }}
  replicas: 2
  {{ else }}
  replicas: {{ .Values.global.dataHighAvailability.replicaCount }}
  {{ end }}
  {{ else }}
  {{ if gt (int $.Values.replicaCount) 1 }}
  replicas: 1
  {{ else }}
  replicas: {{ $.Values.replicaCount | default 1 }}
  {{ end }}
  {{ end }}
  selector:
    matchLabels:
      app: eric-bss-eb-data
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        monitored-by: hpe-csi
        app: eric-bss-eb-data
        {{- include "eric-bss-eb-data.kubernetesIoInfo" . | nindent 8 }}
        eric-bss-eb-network-database-client: 'true'
        eric-bss-eb-network-data-server: 'true'
        eric-bss-eb-network-data-client: 'true'
        eric-bss-eb-network-lem-client: 'true'
        eric-bss-eb-network-vault-client: 'true'
      annotations:
      {{- include "eric-bss-eb-data.productInfo" . | indent 8 }}
    spec:
      {{- if $.Values.affinity }}
      affinity: {{ toYaml $.Values.affinity | nindent 8 }}
      {{- else if $.Values.global.affinity }}
      affinity: {{ toYaml $.Values.global.affinity | nindent 8 }}
      {{- end }}
      securityContext:
        {{- include "eric-bss-eb-data.checkPodUserLoginData" . | nindent 8 }}
      {{ if not .Values.maintenanceMode }}
      {{- if $.Values.global.imageCredentials.pullSecret }}
      imagePullSecrets:
        - name: {{ $.Values.global.imageCredentials.pullSecret }}
      {{- end }}
      initContainers:
        - name: data-init
          # containerKey identifies the section inside .Values.images and .Values.imageCredentials to be used here
          {{ $templDict := dict "Values" .Values "containerKey" "data-init" }}
          image: "{{ template "eric-bss-eb-data.registryUrl" $templDict }}/{{ template "eric-bss-eb-data.repoPath" $templDict }}{{ index .Values "images" $templDict.containerKey "name" }}:{{ index .Values "images" $templDict.containerKey "tag" }}"
          imagePullPolicy: {{ template "eric-bss-eb-data.pullPolicy" $templDict }}
          env:
          - name: K8S_ENV
            value: "true"
          - name: TZ
            value: "{{ .Values.global.timezone }}"
          {{ if .Values.global.databaseTls.enable }}
          - name: ORA_SEC_WLT
            value: "/oratns/secret"
          {{ end }}
          {{ if not $.Values.global.secretManager.enable }}
          - name: NO_SECRET_MANAGER
            value: "true"
          {{ end }}
          command: ["/bin/bash"]
          args: ["-c","printcfg H DXL_PROFILE INIT_STATUS_IND DXL_PROFILE_ID>/pod-local/stopped_profiles"]
          #command: ["tail"]
          #args: ["-f", "/dev/null"]
          volumeMounts:
          - mountPath: /workdir/PWDMGR_SECURITY
            name: pwdmgr-security-volume
          - name: vault-clientcfg-xml-volume
            mountPath: /ericssonbilling/resource/vault-clientcfg/vault-clientcfg.xml
            subPath: vault-clientcfg.xml
          - name: vault-clientcfg-dtd-volume
            mountPath: /ericssonbilling/resource/vault-clientcfg/vault-clientcfg.dtd
            subPath: vault-clientcfg.dtd
          - mountPath: /log
            name: logs-volume
          - name: tnsnames-ora-volume
            mountPath: /oratns/tnsnames.ora
            subPath: tnsnames.ora
          - name: sqlnet-ora-volume
            mountPath: /oratns/sqlnet.ora
            subPath: sqlnet.ora
          {{ if .Values.global.databaseTls.enable }}
          - name: wallet-volume
            mountPath: /oratns/secret
          {{ end }}
          {{ if and ($.Values.global.secretManager.enable) ($.Values.global.secretManager.vault.caSecretName) }}
          - name: secretmanager-tls-volume
            mountPath: "/tls/secretManager/vault"
            readOnly: true
          {{ end }}
          - mountPath: /pod-local
            name: pod-local-volume
      {{ end }}
      {{- if .Values.global.rbac.sa_roles_create }}
      serviceAccountName: {{ .Chart.Name }}-sa
      {{- end }}
      containers:
        - name: eric-bss-eb-data
          # containerKey identifies the section inside .Values.images and .Values.imageCredentials to be used here
          {{ $templDict := dict "Values" .Values "containerKey" "data" }}
          image: "{{ template "eric-bss-eb-data.registryUrl" $templDict }}/{{ template "eric-bss-eb-data.repoPath" $templDict }}{{ index .Values "images" $templDict.containerKey "name" }}:{{ index .Values "images" $templDict.containerKey "tag" }}"
          imagePullPolicy: {{ template "eric-bss-eb-data.pullPolicy" $templDict }}
          resources:
            limits:
              memory: "{{ .Values.resources.limits.memory}}"
              cpu: "{{ .Values.resources.limits.cpu}}"
            requests:
              memory: "{{ .Values.resources.requests.memory}}"
              cpu: "{{ .Values.resources.requests.cpu}}"
          env:
          - name: DATA_SERVER_ADDRESS_0
            value: "T:eric-bss-eb-data-service:{{ .Values.global.dataClusterPort }}"
          - name: DATA_SERVER_ADDRESS_1
            value: "T:eric-bss-eb-data1-service:{{ .Values.global.dataClusterPort }}"
          - name: DATA_SERVER_WORKDIR_0
            value: "/data-workdir"
          - name: DATA_SERVER_WORKDIR_1
            value: "/data-1-workdir"
          - name: TZ
            value: "{{ .Values.global.timezone }}"
          {{ if .Values.global.securityTls.enable }}
          - name: DATA_SERVER_SSL_ENABLE
            value: "1"
          - name: DATA_CIPHER_LIST
            value: "{{ .Values.global.securityTls.cipherList }}"
          {{ end }}
          {{ if or .Values.dataConfigFile.enable .Values.global.dataHighAvailability.enable }}
          - name: DATA_READ_CONFIG
            value: "FILE"
          - name: DATA_CONFIG_FILE
            value: "/config/data.config"
          {{ end }}
          - name: DATA_INTER_SEND_RETRY
            value: "{{ .Values.envVariables.dataInterSendRetry }}"
          - name: DATA_NON_PERSIST_Q_MAX_BUF_SIZE
            value: "{{ .Values.envVariables.dataNonPersistQMaxBufSize }}"
          - name: DATA_SOCKET_BUFFER_SIZE
            value: "{{ .Values.envVariables.dataSocketBufferSize }}"
          - name: DATA_USE_PERSISTENT_BC_CC
            value: "{{ .Values.envVariables.dataUsePersistentBcCc }}"
          - name: BSCS_WORKDIR
            value: "/data-batch"
          command: ["/bin/bash"]
          {{ if .Values.maintenanceMode }}
          args: ["-c","sleep 36000"]
          {{ else }}
          {{ if .Values.global.dataHighAvailability.enable }}
          # We echo $DATA_SERVER_STOPPED_PROFILES, so that it can be checked easily by "kubectl logs"
          args: ["-c","ulimit -c unlimited && mkdir -p {{ .Values.global.coredumpDirectory }} && \
                       mkdir -p /data-workdir/QUEUES && mkdir -p /data-workdir/TMP && mkdir -p /data-1-workdir/QUEUES && mkdir -p /data-1-workdir/TMP && \
                       mkdir -p /data-fail-in-workdir/QUEUES && export DATA_SERVER_STOPPED_PROFILES=`cat /pod-local/stopped_profiles` && \
                       echo DATA_SERVER_STOPPED_PROFILES=$DATA_SERVER_STOPPED_PROFILES && \
                       . data-env.sh && sleep {{ .Values.waitPeriodSeconds }} && exec ./data --hotstandby --movebad -n {{ .Values.global.dataServerName }}"]
          {{ else }}
          {{ if .Values.dataConfigFile.enable }}
          args: ["-c","ulimit -c unlimited && mkdir -p {{ .Values.global.coredumpDirectory }} && \
                       mkdir -p /data-workdir/QUEUES && mkdir -p /data-workdir/TMP && export DATA_SERVER_STOPPED_PROFILES=`cat /pod-local/stopped_profiles` && \
                       mkdir -p /data-fail-in-workdir/QUEUES && echo DATA_SERVER_STOPPED_PROFILES=$DATA_SERVER_STOPPED_PROFILES && . data-env.sh && \
                       exec ./data --movebad -n {{ .Values.global.dataServerName }}"]
          {{ else }}
          args: ["-c","ulimit -c unlimited && mkdir -p {{ .Values.global.coredumpDirectory }} && \
                       mkdir -p /data-workdir/QUEUES && mkdir -p /data-workdir/TMP && export DATA_SERVER_STOPPED_PROFILES=`cat /pod-local/stopped_profiles` && \
                       echo DATA_SERVER_STOPPED_PROFILES=$DATA_SERVER_STOPPED_PROFILES && . data-env.sh && exec ./data --movebad"]
          {{ end }}
          {{ end }}
          {{ end }}
          #command: ["tail"]
          #args: ["-f", "/dev/null"]
          {{ if not .Values.maintenanceMode }}
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - data-liveness.sh
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
            failureThreshold: {{ .Values.livenessProbe.failureThreshold }}
          {{ end }}
          ports:
            #Note: If TLS connection is enabled DaTA uses dataPort+2
            - name: data
              containerPort: {{ required "A valid .Values.global.dataClusterPort entry is required!" .Values.global.dataClusterPort }}
            - name: data-tls
              containerPort: {{ add .Values.global.dataClusterPort 2 }}
          volumeMounts:
          {{ if .Values.global.securityTls.enable }}
          - name: tls-volume
            mountPath: "/tls/connect"
            readOnly: true
          {{ end }}
          - name: data-workdir-volume
            mountPath: /data-workdir
          {{ if .Values.global.dataHighAvailability.enable }}
          - name: data-1-workdir-volume
            mountPath: /data-1-workdir
          - name: data-fail-in-workdir-volume
            mountPath: /data-fail-in-workdir
          {{ end }}
          - name: data-batch-volume
            mountPath: /data-batch
          - name: logs-volume
            mountPath: /log
          - name: coredump-volume
            mountPath: "{{ .Values.global.coredumpDirectory }}"
          {{ if or .Values.dataConfigFile.enable .Values.global.dataHighAvailability.enable }}
          - name: data-config-volume
            mountPath: /config/data.config
            subPath: data.config
          {{ end }}
          - name: pod-local-volume
            mountPath: /pod-local
          # Delay termination of DaTA pod by using preStop hook to avoid issues in following cases:
          # 1. Stop and restart DaTA pod only: the endpoint in DaTA service may not be updated yet
          #     while the DaTA pod has already been deleted
          # 2. In case of node move and shutdown, DaTA is terminated at first and other applications
          #     may try to reconnect to DaTA before their termination, which can cause long termination time.
          lifecycle:
            preStop:
              exec:
                command: ["/bin/sleep","1"]
        {{ if .Values.global.dataHighAvailability.enable }}
        - name: eric-bss-eb-data-monitor
          # containerKey identifies the section inside .Values.images and .Values.imageCredentials to be used here
          {{ $templDict := dict "Values" .Values "containerKey" "data-monitor" }}
          image: "{{ template "eric-bss-eb-data.registryUrl" $templDict }}/{{ template "eric-bss-eb-data.repoPath" $templDict }}{{ index .Values "images" $templDict.containerKey "name" }}:{{ index .Values "images" $templDict.containerKey "tag" }}"
          imagePullPolicy: {{ template "eric-bss-eb-data.pullPolicy" $templDict }}
          resources:
            limits:
              memory: "{{ .Values.resources.limits.memory}}"
              cpu: "{{ .Values.resources.limits.cpu}}"
            requests:
              memory: "{{ .Values.resources.requests.memory}}"
              cpu: "{{ .Values.resources.requests.cpu}}"
          env:
          - name: DATA_SERVER_ADDRESS_0
            value: "T:eric-bss-eb-data-service:{{ .Values.global.dataClusterPort }}"
          - name: DATA_SERVER_ADDRESS_1
            value: "T:eric-bss-eb-data1-service:{{ .Values.global.dataClusterPort }}"
          - name: TZ
            value: "{{ .Values.global.timezone }}"
          - name: BSCS_WORKDIR
            value: "/data-batch"
          - name: DATA_HA_MODE
            value: "1"
          - name: LEASE_DURATION_SECONDS
            value: "{{ .Values.dataMonitor.leaseDurationSeconds }}"
          - name: TAKEOVER_TIME_PERIOD
            value: "{{ .Values.dataMonitor.takeOverTimeoutPeriod }}"
          - name: KEEP_ALIVE_SCALE_FACTOR
            value: "{{ .Values.dataMonitor.keepAliveScaleFactor }}"
          - name: RETRY_SCALE_FACTOR
            value: "{{ .Values.dataMonitor.retryScaleFactor }}"
          - name: GRACE_PERIOD
            value: "{{ .Values.terminationGracePeriodSeconds }}"
          {{ if .Values.global.securityTls.enable }}
          - name: DATA_SSL_CONNECT
            value: "1"
          - name: DATA_CIPHER_LIST
            value: "{{ $.Values.global.securityTls.cipherList }}"
          {{ end }}
          command: ["/bin/bash"]
          args: ["-c","ulimit -c unlimited && mkdir -p {{ .Values.global.coredumpDirectory }} && mkdir -p /log/data-monitor && . data-env.sh && ./dataMonitor.sh "]
          livenessProbe:
            exec:
              command: ["/bin/bash","-c","ps -e | grep dataMonitor.sh"]
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
            failureThreshold: {{ .Values.livenessProbe.failureThreshold }}
          ports:
            #Note: If TLS connection is enabled DaTA uses dataPort+2
            - name: data
              containerPort: {{ required "A valid .Values.global.dataClusterPort entry is required!" .Values.global.dataClusterPort }}
            - name: data-tls
              containerPort: {{ add .Values.global.dataClusterPort 2 }}
          volumeMounts:
          {{ if .Values.global.securityTls.enable }}
          - name: tls-volume-monitor
            mountPath: "/tls/connect"
            readOnly: true
          {{ end }}
          - name: logs-volume
            mountPath: /log
          - name: coredump-volume
            mountPath: "{{ .Values.global.coredumpDirectory }}"
        {{ end }}
      {{- if .Values.nodeSelector }}
      nodeSelector: {{ toYaml .Values.nodeSelector | nindent 8 }}
      {{- else if .Values.global.nodeSelector }}
      nodeSelector: {{ toYaml .Values.global.nodeSelector | nindent 8 }}
      {{- end }}
      terminationGracePeriodSeconds: {{ .Values.terminationGracePeriodSeconds }}
      restartPolicy: "{{ .Values.restartPolicy }}"
      volumes:
      {{ if .Values.global.securityTls.enable }}
      - name: tls-volume
        secret:
          secretName: "{{ .Values.global.securityTls.serverSecretName }}"
          items:
          - key: {{ .Values.global.securityTls.dataServerKeystoreKey }}
            path: certificate.p12
          - key: {{ .Values.global.securityTls.dataKeystorePasswordKey }}
            path: keystorePassword.txt
      {{ if .Values.global.dataHighAvailability.enable }}
      - name: tls-volume-monitor
        secret:
          secretName: "{{ $.Values.global.securityTls.clientSecretName }}"
          items:
            - key: {{ $.Values.global.securityTls.trustedCertificateKey }}
              path: certificate.pem
      {{ end }}
      {{ end }}
      {{ if and ($.Values.global.secretManager.enable) ($.Values.global.secretManager.vault.caSecretName) }}
      - name: secretmanager-tls-volume
        secret:
          secretName: "{{ .Values.global.secretManager.vault.caSecretName }}"
          items:
          - key: {{ .Values.global.secretManager.vault.caCertificate }}
            path: ca.pem
      {{ end }}
      {{ if .Values.global.databaseTls.enable }}
      - name: wallet-volume
        secret:
          secretName: "{{ .Values.global.databaseTls.clientSecretName }}"
      {{ end }}
      - name: data-workdir-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-data-workdir
      {{ if .Values.global.dataHighAvailability.enable }}
      - name: data-1-workdir-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-data-1-workdir
      - name: data-fail-in-workdir-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-data-fail-in-workdir
      {{ end }}
      - name: data-batch-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-data-batch
      - name: pwdmgr-security-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-pwdmgr-security
      - name: logs-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-logs
      - name: coredump-volume
        persistentVolumeClaim:
          claimName: eric-bss-eb-pvc-coredump
      - name: tnsnames-ora-volume
        configMap:
          name: eric-bss-eb-data-tnsnames.ora
      - name: sqlnet-ora-volume
        configMap:
          name: eric-bss-eb-data-sqlnet.ora
      {{ if or .Values.dataConfigFile.enable .Values.global.dataHighAvailability.enable }}
      - name: data-config-volume
        configMap:
          name: eric-bss-eb-data.config
      {{ end }}
      - name: vault-clientcfg-xml-volume
        configMap:
          name: eric-bss-eb-data-vault-clientcfg.xml
      - name: vault-clientcfg-dtd-volume
        configMap:
          name: eric-bss-eb-data-vault-clientcfg.dtd
      - name: pod-local-volume
        emptyDir: {}
